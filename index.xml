<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>s3m</title>
    <link>https://s3m.stream/</link>
    <description>Recent content on s3m</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Sep 2020 17:31:52 +0200</lastBuildDate>
    
	<atom:link href="https://s3m.stream/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Install</title>
      <link>https://s3m.stream/post/install/</link>
      <pubDate>Tue, 01 Sep 2020 17:31:52 +0200</pubDate>
      
      <guid>https://s3m.stream/post/install/</guid>
      <description>To install:
cargo install s3m   To install cargo curl https://sh.rustup.rs -sSf | sh
 </description>
    </item>
    
    <item>
      <title>How it works</title>
      <link>https://s3m.stream/post/how/</link>
      <pubDate>Tue, 01 Sep 2020 13:37:16 +0200</pubDate>
      
      <guid>https://s3m.stream/post/how/</guid>
      <description>s3m upload the files in different ways:
 If the input file &amp;lt; buffer size, the file will be uploaded in one-show (not capable of resuming). if the input file &amp;gt; than the buffer size, the file will be uploaded in multi parts and in it can be resumed.  Â The buffer size by default is 10MB (allow to upload up to 100GB) and it can be changed using the option -b.</description>
    </item>
    
    <item>
      <title>The configuration file</title>
      <link>https://s3m.stream/post/config/</link>
      <pubDate>Tue, 01 Sep 2020 13:23:13 +0200</pubDate>
      
      <guid>https://s3m.stream/post/config/</guid>
      <description>By default s3m will try to use the file located at:
~/s3m/config.yml  The format of the file is:
--- hosts: provider_name: region: &amp;lt;region&amp;gt; endpoint: &amp;lt;https://endpoint&amp;gt; access_key: &amp;lt;access_key&amp;gt; secret_key: &amp;lt;secret_key&amp;gt;  For example if you want to use AWS S3 and backblaze:
--- hosts: aws: region: eu-central-1 access_key: ACCESSKEY secret_key: SECRETKEY backblaze: endpoint: s3.us-west-001.backblazeb2.com access_key: ACCESSKEY secret_key: SECRETKEY  Notice the use of endpoint when there is no region
 </description>
    </item>
    
    <item>
      <title>Upload</title>
      <link>https://s3m.stream/post/upload/</link>
      <pubDate>Tue, 01 Sep 2020 13:23:02 +0200</pubDate>
      
      <guid>https://s3m.stream/post/upload/</guid>
      <description>To upload a file to an s3 bucket:
s3m /path_to/file &amp;lt;s3 provider&amp;gt;/&amp;lt;bucket&amp;gt;/file  For example if using this ~/.s3m/config.yml:
--- hosts: aws: region: eu-central-1 access_key: ACCESSKEY secret_key: SECRETKEY backblaze: endpoint: s3.us-west-001.backblazeb2.com access_key: ACCESSKEY secret_key: SECRETKEY To upload a file to aws to a bucket named backup you could do:
s3m /path/to/file aws/backup/file  aws is the S3 provider, backup is the bucket
 PIPE/STDIN You could pipe the output of your application or a file for example:</description>
    </item>
    
    <item>
      <title>List Objects</title>
      <link>https://s3m.stream/post/ls/</link>
      <pubDate>Tue, 01 Sep 2020 13:22:04 +0200</pubDate>
      
      <guid>https://s3m.stream/post/ls/</guid>
      <description>To list the content of a bucket:
s3m ls &amp;lt;s3 provider&amp;gt;/&amp;lt;bucket&amp;gt;  To list available buckets:
s3m ls &amp;lt;s3 provider&amp;gt;/&amp;lt;bucket&amp;gt;  To list in-progress multipart uploads:
s3m ls &amp;lt;s3 provider&amp;gt;/&amp;lt;bucket&amp;gt; -m  </description>
    </item>
    
    <item>
      <title>Delete Objects</title>
      <link>https://s3m.stream/post/rm/</link>
      <pubDate>Tue, 01 Sep 2020 13:22:01 +0200</pubDate>
      
      <guid>https://s3m.stream/post/rm/</guid>
      <description>To remove an object:
s3m rm &amp;lt;s3 provider&amp;gt;/&amp;lt;bucket&amp;gt;/object  To abort a multipart upload:
s3m rm &amp;lt;s3 provider&amp;gt;/&amp;lt;bucket&amp;gt; -a &amp;lt;UploadId&amp;gt;   to get the upload ID you could use s3m ls &amp;lt;s3 provider&amp;gt;/&amp;lt;bucket&amp;gt; -m
 For example to abort all multipart uploads:
s3m ls -m &amp;lt;s3&amp;gt;/&amp;lt;bucket&amp;gt; | awk &#39;{system(&amp;quot;s3m rm &amp;lt;s3&amp;gt;/&amp;lt;bucket&amp;gt;/&amp;quot;$5&amp;quot; -a &amp;quot;$4);}&#39;  </description>
    </item>
    
    <item>
      <title>s3m</title>
      <link>https://s3m.stream/about/</link>
      <pubDate>Fri, 05 Apr 2019 20:50:30 +0200</pubDate>
      
      <guid>https://s3m.stream/about/</guid>
      <description>Problem trying to solve There are streams of data that can not be lost besides that when created, they degrade the performance of running systems, for example, if the stream is a backup of a database, every time the stream is produced it may lock the entire cluster (depends on the options and tools used mysqldump/xtrabackup for example), however, the effort to create the stream is proportional to the size of the database, in most of the cases the bigger the database is the more time and CPU and Memory is required.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://s3m.stream/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://s3m.stream/readme/</guid>
      <description>public www public content</description>
    </item>
    
  </channel>
</rss>